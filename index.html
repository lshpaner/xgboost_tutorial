<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>How XGBoost Works - Interactive Guide</title>
    
    <!-- External Libraries -->
    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjs/11.11.0/math.min.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://d3js.org/d3.v7.min.js"></script>
    
    <!-- AOS Animation Library -->
    <link href="https://unpkg.com/aos@2.3.1/dist/aos.css" rel="stylesheet">
    <script src="https://unpkg.com/aos@2.3.1/dist/aos.js"></script>
    
    <!-- Custom Styles -->
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <!-- Hamburger Menu -->
    <button class="hamburger" id="hamburger">
        <span class="menu-text">☰</span>
        <span class="close-text">×</span>
    </button>

    <!-- Navigation Menu -->
    <nav class="nav-menu" id="navMenu">
        <h3>Navigation</h3>
        <ul>
            <li><a href="#what-is-xgboost">What is XGBoost?</a></li>
            <li><a href="#xgboost-process">The XGBoost Process</a></li>
            <li><a href="#interactive-demo">Interactive Demo</a></li>
            <li><a href="#tree-visualization">Tree Visualization</a></li>
            <li><a href="#sequential-building">Sequential Building</a></li>
            <li><a href="#key-features">Key Features</a></li>
            <li><a href="#mathematical-foundation">Mathematical Foundation</a></li>
            <li><a href="#hyperparameter-tuning">Hyperparameter Tuning</a></li>
            <li><a href="#comparison">vs Other Algorithms</a></li>
            <li><a href="#when-to-use">When to Use XGBoost</a></li>
        </ul>
    </nav>

    <!-- Overlay -->
    <div class="overlay" id="overlay"></div>

    <div class="container">
        <div class="header">
            <h1>How XGBoost Works</h1>
            <p>An Interactive Guide to Extreme Gradient Boosting</p>
        </div>

        <div class="content">
            <!-- Introduction -->
            <div class="section" data-aos="fade-up">
                <h2 id="what-is-xgboost">What is XGBoost?</h2>
                <p>
                    <strong>XGBoost (Extreme Gradient Boosting)</strong> is a powerful machine learning algorithm that builds an ensemble of decision trees sequentially. Each new tree corrects the errors made by the previous trees, resulting in highly accurate predictions.
                </p>
                <div class="key-concept">
                    <h4>Core Idea</h4>
                    <p>
                        Instead of training one complex model, XGBoost trains many simple models (weak learners) and combines them. Each model focuses on correcting the mistakes of the previous ones, leading to a strong overall predictor.
                    </p>
                </div>
            </div>

            <!-- Step-by-step Process -->
            <div class="section" data-aos="fade-up">
                <h2 id="xgboost-process">The XGBoost Process</h2>
                <div class="step-indicator">
                    <div class="step active">
                        <div class="step-number">1</div>
                        <div class="step-label">Initialize</div>
                    </div>
                    <div class="step">
                        <div class="step-number">2</div>
                        <div class="step-label">Compute Gradients</div>
                    </div>
                    <div class="step">
                        <div class="step-number">3</div>
                        <div class="step-label">Build Tree</div>
                    </div>
                    <div class="step">
                        <div class="step-number">4</div>
                        <div class="step-label">Update Predictions</div>
                    </div>
                    <div class="step">
                        <div class="step-number">5</div>
                        <div class="step-label">Repeat</div>
                    </div>
                </div>

                <h3>Step 1: Initialize Predictions</h3>
                <p>Start with a simple initial prediction (often the mean for regression or log-odds for classification).</p>
                <div class="formula">$$F_0(x) = \arg\min_{\gamma} \sum_{i=1}^{n} L(y_i, \gamma)$$</div>

                <h3>Step 2: Compute Gradients and Hessians</h3>
                <p>Calculate the first and second derivatives of the loss function with respect to predictions:</p>
                <div class="formula">
$$g_i = \frac{\partial L(y_i, \hat{y}_i)}{\partial \hat{y}_i} \quad \text{(gradient)}$$
$$h_i = \frac{\partial^2 L(y_i, \hat{y}_i)}{\partial \hat{y}_i^2} \quad \text{(hessian)}$$
                </div>

                <h3>Step 3: Build a New Tree</h3>
                <p>Grow a decision tree by finding splits that minimize the objective function:</p>
                <div class="formula">
$$\text{Gain} = \frac{1}{2}\left[\frac{G_L^2}{H_L + \lambda} + \frac{G_R^2}{H_R + \lambda} - \frac{(G_L + G_R)^2}{H_L + H_R + \lambda}\right] - \gamma$$
                </div>
                <p>Where \(G\) and \(H\) are sums of gradients and hessians, \(L\) and \(R\) denote left and right splits.</p>

                <h3>Step 4: Update Predictions</h3>
                <p>Add the new tree's predictions with a learning rate:</p>
                <div class="formula">$$F_m(x) = F_{m-1}(x) + \eta \cdot f_m(x)$$</div>

                <h3>Step 5: Repeat</h3>
                <p>Continue building trees until reaching the maximum number or convergence.</p>
            </div>

            <!-- Interactive Demo -->
            <div class="section" data-aos="fade-up">
                <h2 id="interactive-demo">Interactive Demo: Regression Example</h2>
                <div class="interactive-demo">
                    <p style="margin-bottom: 20px;">
                        Adjust the parameters below to see how XGBoost learns a non-linear function. Watch how each boosting round reduces the error!
                    </p>
                    
                    <div class="controls">
                        <div class="control-group">
                            <label for="n-estimators">Number of Trees:</label>
                            <input type="range" id="n-estimators" min="1" max="20" value="5" step="1">
                            <span id="n-estimators-value">5</span>
                        </div>
                        <div class="control-group">
                            <label for="learning-rate">Learning Rate (η):</label>
                            <input type="range" id="learning-rate" min="0.01" max="1" value="0.3" step="0.01">
                            <span id="learning-rate-value">0.30</span>
                        </div>
                        <div class="control-group">
                            <label for="max-depth">Max Tree Depth:</label>
                            <input type="range" id="max-depth" min="1" max="6" value="3" step="1">
                            <span id="max-depth-value">3</span>
                        </div>
                        <button onclick="runXGBoostDemo()">Train Model</button>
                    </div>

                    <div class="metrics-display" id="metrics-display"></div>
                </div>
                
                <div class="plot-grid" style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 15px; margin-top: 20px;">
                    <div style="background: white; border-radius: 8px; padding: 15px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);" data-aos="zoom-in" data-aos-delay="100">
                        <div id="predictions-plot"></div>
                    </div>
                    <div style="background: white; border-radius: 8px; padding: 15px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);" data-aos="zoom-in" data-aos-delay="200">
                        <div id="residuals-plot"></div>
                    </div>
                    <div style="background: white; border-radius: 8px; padding: 15px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);" data-aos="zoom-in" data-aos-delay="300">
                        <div id="learning-curve"></div>
                    </div>
                </div>
            </div>

            <!-- Tree Visualization Section -->
            <div class="section" data-aos="fade-up">
                <h2 id="tree-visualization">Tree Structure Visualization</h2>
                <p>
                    Below you can see the actual decision trees built by XGBoost. Each tree makes binary splits on features to partition the data and assign predictions to leaf nodes.
                </p>
                
                <div class="trees-grid" id="trees-grid"></div>
            </div>

            <!-- Sequential Building Section -->
            <div class="section" data-aos="fade-up">
                <h2 id="sequential-building">Sequential Tree Building Process</h2>
                <p>
                    Step through the boosting process to see how each tree is added sequentially. Watch how predictions improve and residuals shrink with each new tree.
                </p>
                
                <div class="step-controls">
                    <button id="prev-btn" onclick="previousStep()">← Previous</button>
                    <span class="current-step" id="step-display">Step 0: Initial Prediction</span>
                    <button id="next-btn" onclick="nextStep()">Next →</button>
                </div>

                <div class="sequential-grid" style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-top: 20px;">
                    <div class="tree-container" id="sequential-tree-view"></div>
                    <div class="tree-container" id="sequential-predictions-plot"></div>
                </div>
            </div>

            <!-- Key Features -->
            <div class="section" data-aos="fade-up">
                <h2 id="key-features">Key Features of XGBoost</h2>
                
                <h3>1. Regularization</h3>
                <p>XGBoost adds L1 and L2 regularization to prevent overfitting:</p>
                <div class="formula">
$$\text{Objective} = \sum_{i=1}^{n} L(y_i, \hat{y}_i) + \sum_{k=1}^{K} \Omega(f_k)$$
$$\text{where } \Omega(f) = \gamma T + \frac{1}{2}\lambda \sum_{j=1}^{T} w_j^2$$
                </div>
                <p>Here, \(\gamma\) penalizes the number of leaves (\(T\)), and \(\lambda\) penalizes large leaf weights (\(w\)).</p>

                <h3>2. Tree Pruning</h3>
                <p>Uses a "max_depth" parameter and prunes trees backward, removing splits that don't provide sufficient gain.</p>

                <h3>3. Handling Missing Values</h3>
                <p>Automatically learns the best direction to handle missing values during tree building.</p>

                <h3>4. Parallel Processing</h3>
                <p>Parallelizes tree construction by sorting features beforehand and using multiple threads.</p>

                <h3>5. Column Subsampling</h3>
                <p>Randomly samples features for each tree (like Random Forests) to reduce overfitting and speed up training.</p>
            </div>

            <!-- Mathematical Foundation -->
            <div class="section" data-aos="fade-up">
                <h2 id="mathematical-foundation">Mathematical Foundation</h2>
                
                <div class="key-concept">
                    <h4>Taylor Expansion Approximation</h4>
                    <p>
                        XGBoost uses a second-order Taylor expansion to approximate the loss function. This allows for faster optimization:
                    </p>
                    <div class="formula">
$$L(y_i, \hat{y}_i^{(t)}) \approx L(y_i, \hat{y}_i^{(t-1)}) + g_i f_t(x_i) + \frac{1}{2}h_i f_t^2(x_i)$$
                    </div>
                    <p>
                        This quadratic approximation makes the optimization problem convex and solvable in closed form for each leaf.
                    </p>
                </div>

                <h3>Optimal Leaf Weight</h3>
                <p>For each leaf \(j\), the optimal weight is calculated as:</p>
                <div class="formula">
$$w_j^* = -\frac{\sum_{i \in I_j} g_i}{\sum_{i \in I_j} h_i + \lambda}$$
                </div>
                <p>Where \(I_j\) represents the set of instances in leaf \(j\).</p>
            </div>

            <!-- Practical Tips -->
            <div class="section" data-aos="fade-up">
                <h2 id="hyperparameter-tuning">Hyperparameter Tuning Tips</h2>
                
                <div class="info-box">
                    <strong>Key Parameters to Tune:</strong>
                </div>

                <h3>Tree-Specific Parameters</h3>
                <ul style="margin-left: 20px; line-height: 2;">
                    <li><code>max_depth</code>: Controls tree depth (3-10 typically works well)</li>
                    <li><code>min_child_weight</code>: Minimum sum of instance weight in a child (prevents overfitting)</li>
                    <li><code>gamma</code>: Minimum loss reduction for a split (regularization)</li>
                </ul>

                <h3>Boosting Parameters</h3>
                <ul style="margin-left: 20px; line-height: 2;">
                    <li><code>n_estimators</code>: Number of trees (50-1000+)</li>
                    <li><code>learning_rate</code>: Step size shrinkage (0.01-0.3)</li>
                    <li><code>subsample</code>: Fraction of samples for each tree (0.5-1.0)</li>
                    <li><code>colsample_bytree</code>: Fraction of features for each tree (0.5-1.0)</li>
                </ul>

                <h3>Regularization Parameters</h3>
                <ul style="margin-left: 20px; line-height: 2;">
                    <li><code>lambda</code> (L2): Ridge regularization (default 1)</li>
                    <li><code>alpha</code> (L1): Lasso regularization (default 0)</li>
                </ul>

                <div class="key-concept" style="margin-top: 30px;">
                    <h4>General Strategy</h4>
                    <p>
                        Start with a low learning rate (0.1) and many trees. Increase max_depth gradually. 
                        Add regularization (subsample, colsample_bytree) if overfitting. Use early stopping 
                        with a validation set to find the optimal number of trees.
                    </p>
                </div>
            </div>

            <!-- Comparison -->
            <div class="section" data-aos="fade-up">
                <h2 id="comparison">XGBoost vs Other Algorithms</h2>
                <table class="comparison-table">
                    <thead>
                        <tr style="background: #2c3e50; color: white;">
                            <th style="padding: 15px; text-align: left;">Feature</th>
                            <th style="padding: 15px; text-align: left;">XGBoost</th>
                            <th style="padding: 15px; text-align: left;">Random Forest</th>
                            <th style="padding: 15px; text-align: left;">Gradient Boosting</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr style="background: #f8f9fa;">
                            <td style="padding: 12px; border: 1px solid #ddd;"><strong>Training</strong></td>
                            <td style="padding: 12px; border: 1px solid #ddd;" data-label="XGBoost: ">Sequential</td>
                            <td style="padding: 12px; border: 1px solid #ddd;" data-label="Random Forest: ">Parallel</td>
                            <td style="padding: 12px; border: 1px solid #ddd;" data-label="Gradient Boosting: ">Sequential</td>
                        </tr>
                        <tr>
                            <td style="padding: 12px; border: 1px solid #ddd;"><strong>Regularization</strong></td>
                            <td style="padding: 12px; border: 1px solid #ddd;" data-label="XGBoost: "><span class="checkmark">✓</span> Built-in L1/L2</td>
                            <td style="padding: 12px; border: 1px solid #ddd;" data-label="Random Forest: "><span class="xmark">✗</span> Limited</td>
                            <td style="padding: 12px; border: 1px solid #ddd;" data-label="Gradient Boosting: "><span class="xmark">✗</span> Limited</td>
                        </tr>
                        <tr style="background: #f8f9fa;">
                            <td style="padding: 12px; border: 1px solid #ddd;"><strong>Speed</strong></td>
                            <td style="padding: 12px; border: 1px solid #ddd;" data-label="XGBoost: ">Fast (optimized)</td>
                            <td style="padding: 12px; border: 1px solid #ddd;" data-label="Random Forest: ">Fast (parallel)</td>
                            <td style="padding: 12px; border: 1px solid #ddd;" data-label="Gradient Boosting: ">Slower</td>
                        </tr>
                        <tr>
                            <td style="padding: 12px; border: 1px solid #ddd;"><strong>Missing Values</strong></td>
                            <td style="padding: 12px; border: 1px solid #ddd;" data-label="XGBoost: "><span class="checkmark">✓</span> Automatic handling</td>
                            <td style="padding: 12px; border: 1px solid #ddd;" data-label="Random Forest: "><span class="xmark">✗</span> Needs preprocessing</td>
                            <td style="padding: 12px; border: 1px solid #ddd;" data-label="Gradient Boosting: "><span class="xmark">✗</span> Needs preprocessing</td>
                        </tr>
                        <tr style="background: #f8f9fa;">
                            <td style="padding: 12px; border: 1px solid #ddd;"><strong>Overfitting Risk</strong></td>
                            <td style="padding: 12px; border: 1px solid #ddd;" data-label="XGBoost: ">Low (with regularization)</td>
                            <td style="padding: 12px; border: 1px solid #ddd;" data-label="Random Forest: ">Low (ensemble)</td>
                            <td style="padding: 12px; border: 1px solid #ddd;" data-label="Gradient Boosting: ">Higher</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <!-- When to Use XGBoost -->
            <div class="section" data-aos="fade-up">
                <h2 id="when-to-use">When to Use XGBoost</h2>
                
                <div class="key-concept">
                    <h4>XGBoost excels in these scenarios:</h4>
                    <ul style="margin-left: 20px; line-height: 2;">
                        <li><strong>Structured/tabular data:</strong> XGBoost is one of the top choices for datasets with rows and columns (not images or text)</li>
                        <li><strong>Medium-to-large datasets:</strong> Works well with thousands to millions of samples</li>
                        <li><strong>Mixed feature types:</strong> Handles both numerical and categorical features effectively</li>
                        <li><strong>Competition/production:</strong> Industry standard for Kaggle competitions and production ML pipelines</li>
                        <li><strong>When you need interpretability:</strong> Feature importance and SHAP values provide model explanations</li>
                        <li><strong>Imbalanced datasets:</strong> Built-in support for handling class imbalance</li>
                    </ul>
                </div>

                <div class="info-box">
                    <strong>Consider alternatives when:</strong>
                    <ul style="margin-left: 20px; margin-top: 10px; line-height: 1.8;">
                        <li>Working with images, audio, or text (use deep learning instead)</li>
                        <li>You need real-time predictions with minimal latency (simpler models may be faster)</li>
                        <li>Dataset is very small (&lt;100 samples) - risk of overfitting</li>
                        <li>Linear relationships dominate (logistic/linear regression may suffice)</li>
                    </ul>
                </div>
            </div>
        </div>

        <div style="text-align: center; padding: 30px; background: #ecf0f1; color: #7f8c8d; font-size: 0.9em; border-top: 1px solid #bdc3c7;">
            © 2026 Leon Shpaner. All rights reserved.
        </div>
    </div>

    <!-- Custom JavaScript -->
    <script src="script.js"></script>
</body>
</html>